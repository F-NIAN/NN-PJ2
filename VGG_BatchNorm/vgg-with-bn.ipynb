{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nUtils for neural networks\n\"\"\"\n\nfrom torch import nn\n\n\ndef init_weights_(m):\n    \"\"\"\n    Initializes weights of m according to Xavier normal method.\n\n    :param m: module\n    :return:\n    \"\"\"\n    if isinstance(m, nn.Conv2d):\n        nn.init.xavier_normal_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.ones_(m.weight)\n        nn.init.zeros_(m.bias)\n\n    elif isinstance(m, nn.BatchNorm1d):\n        nn.init.ones_(m.weight)\n        nn.init.zeros_(m.bias)\n\n    elif isinstance(m, nn.Linear):\n        nn.init.xavier_normal_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T14:34:09.299062Z","iopub.execute_input":"2025-06-07T14:34:09.299707Z","iopub.status.idle":"2025-06-07T14:34:13.530704Z","shell.execute_reply.started":"2025-06-07T14:34:09.299673Z","shell.execute_reply":"2025-06-07T14:34:13.530060Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nData loaders\n\"\"\"\nimport matplotlib as mpl\nmpl.use('Agg')\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torchvision.datasets as datasets\n\n\n\nclass PartialDataset(Dataset):\n    def __init__(self, dataset, n_items=10):\n        self.dataset = dataset\n        self.n_items = n_items\n\n    def __getitem__(self):\n        return self.dataset.__getitem__()\n\n    def __len__(self):\n        return min(self.n_items, len(self.dataset))\n\n\ndef get_cifar_loader(root='../data/', batch_size=128, train=True, shuffle=True, num_workers=4, n_items=-1):\n    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                     std=[0.5, 0.5, 0.5])\n\n    data_transforms = transforms.Compose(\n        [transforms.ToTensor(),\n        normalize])\n\n    dataset = datasets.CIFAR10(root=root, train=train, download=True, transform=data_transforms)\n    if n_items > 0:\n        dataset = PartialDataset(dataset, n_items)\n\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n\n    return loader\n\nif __name__ == '__main__':\n    train_loader = get_cifar_loader()\n    for X, y in train_loader:\n        print(X[0])\n        print(y[0])\n        print(X[0].shape)\n        img = np.transpose(X[0], [1,2,0])\n        plt.imshow(img*0.5 + 0.5)\n        plt.savefig('sample.png')\n        print(X[0].max())\n        print(X[0].min())\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T14:34:13.532086Z","iopub.execute_input":"2025-06-07T14:34:13.532473Z","iopub.status.idle":"2025-06-07T14:34:24.081007Z","shell.execute_reply.started":"2025-06-07T14:34:13.532442Z","shell.execute_reply":"2025-06-07T14:34:24.080153Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:02<00:00, 79.4MB/s] \n","output_type":"stream"},{"name":"stdout","text":"tensor([[[-0.3961, -0.5059, -0.5765,  ..., -0.9608, -0.9059, -0.9294],\n         [-0.6784, -0.7882, -0.5765,  ..., -0.8510, -0.9216, -0.8902],\n         [-0.7882, -0.9373, -0.8431,  ..., -0.7333, -0.9373, -0.8118],\n         ...,\n         [-0.8431, -0.8039, -0.7882,  ..., -0.6314, -0.7020, -0.7255],\n         [-0.8510, -0.8275, -0.8196,  ..., -0.6627, -0.7176, -0.7569],\n         [-0.8667, -0.8431, -0.8353,  ..., -0.6941, -0.7333, -0.7725]],\n\n        [[-0.3333, -0.4902, -0.5922,  ..., -0.9373, -0.9216, -0.9373],\n         [-0.6235, -0.7490, -0.5608,  ..., -0.8275, -0.9373, -0.8902],\n         [-0.8196, -0.9451, -0.8353,  ..., -0.7490, -0.9451, -0.8118],\n         ...,\n         [-0.8275, -0.7882, -0.7725,  ..., -0.5922, -0.6549, -0.6863],\n         [-0.8353, -0.8118, -0.8039,  ..., -0.6314, -0.6863, -0.7255],\n         [-0.8510, -0.8275, -0.8196,  ..., -0.6627, -0.7020, -0.7490]],\n\n        [[-0.3647, -0.6235, -0.7020,  ..., -0.9529, -0.9373, -0.9216],\n         [-0.6471, -0.8039, -0.6627,  ..., -0.7804, -0.9608, -0.9059],\n         [-0.8588, -0.9686, -0.8588,  ..., -0.7098, -0.9608, -0.8431],\n         ...,\n         [-0.8353, -0.7961, -0.7804,  ..., -0.6078, -0.6706, -0.6941],\n         [-0.8431, -0.8196, -0.8118,  ..., -0.6392, -0.6941, -0.7333],\n         [-0.8588, -0.8353, -0.8275,  ..., -0.6706, -0.7098, -0.7569]]])\ntensor(1)\ntorch.Size([3, 32, 32])\ntensor(1.)\ntensor(-1.)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nVGG\n\"\"\"\nimport numpy as np\nfrom torch import nn\n\n# ## Models implementation\ndef get_number_of_parameters(model):\n    parameters_n = 0\n    for parameter in model.parameters():\n        parameters_n += np.prod(parameter.shape).item()\n\n    return parameters_n\n\nclass VGG_A_BatchNorm(nn.Module):\n    def __init__(self, inp_ch=3, num_classes=10, init_weights=True):\n        super().__init__()\n        \n        self.features = nn.Sequential(\n            # stage 1\n            nn.Conv2d(in_channels=inp_ch, out_channels=64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # stage 2\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # stage 3\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # stage 4\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # stage5\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 1 * 1, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, num_classes))\n        \n        if init_weights:\n            self._init_weights()\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x.view(-1, 512 * 1 * 1))\n        return x\n    \n    def _init_weights(self):\n        for m in self.modules():\n            init_weights_(m)\n\nclass VGG_A(nn.Module):\n    \"\"\"VGG_A model\n\n    size of Linear layers is smaller since input assumed to be 32x32x3, instead of\n    224x224x3\n    \"\"\"\n\n    def __init__(self, inp_ch=3, num_classes=10, init_weights=True):\n        super().__init__()\n\n        self.features = nn.Sequential(\n            # stage 1\n            nn.Conv2d(in_channels=inp_ch, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # stage 2\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # stage 3\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # stage 4\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # stage5\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 1 * 1, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, num_classes))\n\n        if init_weights:\n            self._init_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x.view(-1, 512 * 1 * 1))\n        return x\n\n    def _init_weights(self):\n        for m in self.modules():\n            init_weights_(m)\n\n\nclass VGG_A_Light(nn.Module):\n    def __init__(self, inp_ch=3, num_classes=10):\n        super().__init__()\n\n        self.stage1 = nn.Sequential(\n            nn.Conv2d(in_channels=inp_ch, out_channels=16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.stage2 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        '''\n        self.stage3 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.stage4 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.stage5 = nn.Sequential(\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        '''\n        self.classifier = nn.Sequential(\n            nn.Linear(32 * 8 * 8, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes))\n\n    def forward(self, x):\n        x = self.stage1(x)\n        x = self.stage2(x)\n        # x = self.stage3(x)\n        # x = self.stage4(x)\n        # x = self.stage5(x)\n        x = self.classifier(x.view(-1, 32 * 8 * 8))\n        return x\n\n\nclass VGG_A_Dropout(nn.Module):\n    def __init__(self, inp_ch=3, num_classes=10):\n        super().__init__()\n\n        self.stage1 = nn.Sequential(\n            nn.Conv2d(in_channels=inp_ch, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.stage2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.stage3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.stage4 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.stage5 = nn.Sequential(\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(512 * 1 * 1, 512),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(512, 512),\n            nn.ReLU(True),\n            nn.Linear(512, num_classes))\n\n    def forward(self, x):\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.stage5(x)\n        x = self.classifier(x.view(-1, 512 * 1 * 1))\n        return x\n\n\nif __name__ == '__main__':\n    print(get_number_of_parameters(VGG_A()))\n    print(get_number_of_parameters(VGG_A_Light()))\n    print(get_number_of_parameters(VGG_A_Dropout()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T14:34:24.082203Z","iopub.execute_input":"2025-06-07T14:34:24.082568Z","iopub.status.idle":"2025-06-07T14:34:24.336042Z","shell.execute_reply.started":"2025-06-07T14:34:24.082539Z","shell.execute_reply":"2025-06-07T14:34:24.335182Z"}},"outputs":[{"name":"stdout","text":"9750922\n285162\n9750922\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import matplotlib as mpl\nmpl.use('Agg')  # Use non-interactive backend for matplotlib\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport numpy as np\nimport torch\nimport os\nimport random\nfrom tqdm import tqdm as tqdm\nimport copy\nimport time\nfrom collections import defaultdict\n\n# Import your custom modules\n# from models.vgg import VGG_A\n# from models.vgg import VGG_A_BatchNorm\n# from data.loaders import get_cifar_loader\n\n# Initialize parameters\ndevice_id = [0,1]\nnum_workers = 4\nbatch_size = 128\n\n# Kaggle output paths\nfigures_path = '/kaggle/working/figures'\nmodels_path = '/kaggle/working/models'\nos.makedirs(figures_path, exist_ok=True)\nos.makedirs(models_path, exist_ok=True)\n\n# Device configuration\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nprint(f\"GPU count: {torch.cuda.device_count()}\")\n\n# Data loaders\ntrain_loader = get_cifar_loader(train=True, batch_size=batch_size, num_workers=num_workers)\nval_loader = get_cifar_loader(train=False, batch_size=batch_size, num_workers=num_workers)\n\n# Accuracy calculation\ndef get_accuracy(model, loader):\n    correct = 0\n    total = 0\n    model.eval()\n    with torch.no_grad():\n        for data in loader:\n            x, y = data\n            x, y = x.to(device), y.to(device)\n            outputs = model(x)\n            _, predicted = torch.max(outputs.data, 1)\n            total += y.size(0)\n            correct += (predicted == y).sum().item()\n    return correct / total\n\n# Loss calculation\ndef get_loss(model, loader, criterion):\n    total_loss = 0\n    total_samples = 0\n    model.eval()\n    with torch.no_grad():\n        for data in loader:\n            x, y = data\n            x, y = x.to(device), y.to(device)\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            total_loss += loss.item() * x.size(0)\n            total_samples += x.size(0)\n    return total_loss / total_samples\n\n# Seed setup\ndef set_random_seeds(seed_value=0):\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    random.seed(seed_value)\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# Training function with full metrics tracking\ndef train(model, optimizer, criterion, train_loader, val_loader, scheduler=None, \n          epochs_n=100, model_save_path=None, lr=None, model_type=None):\n    model.to(device)\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model, device_ids=device_id)\n    \n    # Initialize metrics storage\n    train_losses = []         # Per-epoch training loss\n    train_accs = []           # Per-epoch training accuracy\n    val_losses = []           # Per-epoch validation loss\n    val_accs = []             # Per-epoch validation accuracy\n    batch_losses = []         # Per-batch training loss\n    learning_rates = []\n    best_val_acc = 0.0\n    best_model_state = None\n\n    # Calculate total batches per epoch\n    batches_per_epoch = len(train_loader)\n    total_batches = batches_per_epoch * epochs_n\n    \n    for epoch in range(epochs_n):\n        epoch_start = time.time()\n        \n        # Training phase\n        model.train()\n        epoch_train_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, data in enumerate(train_loader):\n            x, y = data\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n            \n            # Record per-batch loss\n            batch_loss = loss.item()\n            batch_losses.append(batch_loss)\n            epoch_train_loss += batch_loss * x.size(0)\n            \n            # Calculate batch accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total += y.size(0)\n            correct += (predicted == y).sum().item()\n        \n        # Calculate epoch metrics\n        avg_train_loss = epoch_train_loss / total\n        train_acc = correct / total\n        train_losses.append(avg_train_loss)\n        train_accs.append(train_acc)\n        \n        # Validation phase\n        val_loss = get_loss(model, val_loader, criterion)\n        val_acc = get_accuracy(model, val_loader)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        \n        # Update scheduler if provided\n        if scheduler is not None:\n            scheduler.step()\n            current_lr = optimizer.param_groups[0]['lr']\n        else:\n            current_lr = lr\n        learning_rates.append(current_lr)\n        \n        # Check for best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_state = copy.deepcopy(model.state_dict())\n        \n        epoch_time = time.time() - epoch_start\n        print(f'[{model_type}] Epoch {epoch+1}/{epochs_n} (LR: {current_lr:.0e}) | '\n              f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | '\n              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | '\n              f'Time: {epoch_time:.1f}s')\n    \n    # Save best model\n    if model_save_path and best_model_state is not None:\n        torch.save(best_model_state, model_save_path)\n        print(f'Saved best model to {model_save_path} with val acc {best_val_acc:.4f}')\n    \n    # Return all collected metrics\n    metrics = {\n        'train_losses': train_losses,      # Per-epoch\n        'train_accs': train_accs,          # Per-epoch\n        'val_losses': val_losses,          # Per-epoch\n        'val_accs': val_accs,              # Per-epoch\n        'batch_losses': batch_losses,      # Per-batch\n        'learning_rates': learning_rates,  # Per-epoch\n        'best_val_acc': best_val_acc\n    }\n    return metrics\n\n# Main analysis function\ndef analyze_bn_effect():\n    # Learning rates to test\n    learning_rates = [1e-3, 2e-3, 1e-4, 5e-4]\n    epochs = 20\n    \n    # Store results\n    results_non_bn = {}\n    results_bn = {}\n    \n    # Train non-BN models with different learning rates\n    print(\"\\n\" + \"=\"*50)\n    print(\"Training non-BatchNorm models\")\n    print(\"=\"*50)\n    for lr in learning_rates:\n        print(f\"\\n{'='*20} Training without BN (LR: {lr:.0e}) {'='*20}\")\n        set_random_seeds(2020)\n        model = VGG_A()\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n        model_path = os.path.join(models_path, f'non_bn_model_lr_{lr}.pth')\n        metrics = train(model, optimizer, criterion, train_loader, val_loader, \n                       epochs_n=epochs, model_save_path=model_path, \n                       lr=lr, model_type=\"Without BN\")\n        results_non_bn[lr] = metrics\n    \n    # Train BN models with different learning rates\n    print(\"\\n\" + \"=\"*50)\n    print(\"Training BatchNorm models\")\n    print(\"=\"*50)\n    for lr in learning_rates:\n        print(f\"\\n{'='*20} Training with BN (LR: {lr:.0e}) {'='*20}\")\n        set_random_seeds(2020)\n        model_bn = VGG_A_BatchNorm()\n        optimizer_bn = torch.optim.Adam(model_bn.parameters(), lr=lr)\n        model_path_bn = os.path.join(models_path, f'bn_model_lr_{lr}.pth')\n        metrics_bn = train(model_bn, optimizer_bn, criterion, train_loader, val_loader,\n                          epochs_n=epochs, model_save_path=model_path_bn,\n                          lr=lr, model_type=\"With BN\")\n        results_bn[lr] = metrics_bn\n    \n    # Create performance comparison plots\n    print(\"\\nCreating performance comparison plots...\")\n    create_comparison_plots(results_non_bn, results_bn, learning_rates, epochs)\n    \n    # Create loss landscape plots\n    print(\"\\nCreating loss landscape plots...\")\n    plot_loss_landscape(results_non_bn, results_bn, learning_rates)\n    \n    print(\"\\nAnalysis complete! All outputs saved to /kaggle/working\")\n\n# Create comprehensive comparison plots\ndef create_comparison_plots(results_non_bn, results_bn, learning_rates, epochs):\n    # Create figure for non-BN models\n    plt.figure(figsize=(18, 12))\n    plt.suptitle('VGG Without BatchNorm: Performance Across Learning Rates', fontsize=16)\n    \n    # Plot training loss\n    plt.subplot(2, 2, 1)\n    for lr in learning_rates:\n        metrics = results_non_bn[lr]\n        plt.plot(range(1, epochs+1), metrics['train_losses'], 'o-', label=f'LR={lr:.0e}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Training Loss')\n    plt.title('Training Loss')\n    plt.grid(True)\n    plt.legend()\n    \n    # Plot training accuracy\n    plt.subplot(2, 2, 2)\n    for lr in learning_rates:\n        metrics = results_non_bn[lr]\n        plt.plot(range(1, epochs+1), metrics['train_accs'], 'o-', label=f'LR={lr:.0e}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Training Accuracy')\n    plt.title('Training Accuracy')\n    plt.grid(True)\n    plt.legend()\n    \n    # Plot validation loss\n    plt.subplot(2, 2, 3)\n    for lr in learning_rates:\n        metrics = results_non_bn[lr]\n        plt.plot(range(1, epochs+1), metrics['val_losses'], 'o-', label=f'LR={lr:.0e}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Validation Loss')\n    plt.title('Validation Loss')\n    plt.grid(True)\n    plt.legend()\n    \n    # Plot validation accuracy\n    plt.subplot(2, 2, 4)\n    for lr in learning_rates:\n        metrics = results_non_bn[lr]\n        plt.plot(range(1, epochs+1), metrics['val_accs'], 'o-', label=f'LR={lr:.0e}')\n        # Annotate final accuracy\n        final_acc = metrics['val_accs'][-1]\n        plt.annotate(f'{final_acc:.4f}', (epochs, final_acc), \n                     textcoords=\"offset points\", xytext=(10,0), ha='left')\n    plt.xlabel('Epoch')\n    plt.ylabel('Validation Accuracy')\n    plt.title('Validation Accuracy')\n    plt.grid(True)\n    plt.legend()\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.savefig(os.path.join(figures_path, 'non_bn_performance_comparison.png'), dpi=300)\n    plt.close()\n    \n    # Create figure for BN models\n    plt.figure(figsize=(18, 12))\n    plt.suptitle('VGG With BatchNorm: Performance Across Learning Rates', fontsize=16)\n    \n    # Plot training loss\n    plt.subplot(2, 2, 1)\n    for lr in learning_rates:\n        metrics = results_bn[lr]\n        plt.plot(range(1, epochs+1), metrics['train_losses'], 'o-', label=f'LR={lr:.0e}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Training Loss')\n    plt.title('Training Loss')\n    plt.grid(True)\n    plt.legend()\n    \n    # Plot training accuracy\n    plt.subplot(2, 2, 2)\n    for lr in learning_rates:\n        metrics = results_bn[lr]\n        plt.plot(range(1, epochs+1), metrics['train_accs'], 'o-', label=f'LR={lr:.0e}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Training Accuracy')\n    plt.title('Training Accuracy')\n    plt.grid(True)\n    plt.legend()\n    \n    # Plot validation loss\n    plt.subplot(2, 2, 3)\n    for lr in learning_rates:\n        metrics = results_bn[lr]\n        plt.plot(range(1, epochs+1), metrics['val_losses'], 'o-', label=f'LR={lr:.0e}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Validation Loss')\n    plt.title('Validation Loss')\n    plt.grid(True)\n    plt.legend()\n    \n    # Plot validation accuracy\n    plt.subplot(2, 2, 4)\n    for lr in learning_rates:\n        metrics = results_bn[lr]\n        plt.plot(range(1, epochs+1), metrics['val_accs'], 'o-', label=f'LR={lr:.0e}')\n        # Annotate final accuracy\n        final_acc = metrics['val_accs'][-1]\n        plt.annotate(f'{final_acc:.4f}', (epochs, final_acc), \n                     textcoords=\"offset points\", xytext=(10,0), ha='left')\n    plt.xlabel('Epoch')\n    plt.ylabel('Validation Accuracy')\n    plt.title('Validation Accuracy')\n    plt.grid(True)\n    plt.legend()\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.savefig(os.path.join(figures_path, 'bn_performance_comparison.png'), dpi=300)\n    plt.close()\n    \n    # Create BN vs non-BN comparison at best LR\n    best_lr = max(learning_rates)  # Typically higher LR benefits BN\n    plt.figure(figsize=(15, 10))\n    plt.suptitle(f'BatchNorm vs No BatchNorm Comparison (LR={best_lr:.0e})', fontsize=16)\n    \n    # Training Loss comparison\n    plt.subplot(2, 2, 1)\n    plt.plot(range(1, epochs+1), results_non_bn[best_lr]['train_losses'], 'o-', label='Without BN')\n    plt.plot(range(1, epochs+1), results_bn[best_lr]['train_losses'], 'o-', label='With BN')\n    plt.xlabel('Epoch')\n    plt.ylabel('Training Loss')\n    plt.title('Training Loss Comparison')\n    plt.grid(True)\n    plt.legend()\n    \n    # Training Accuracy comparison\n    plt.subplot(2, 2, 2)\n    plt.plot(range(1, epochs+1), results_non_bn[best_lr]['train_accs'], 'o-', label='Without BN')\n    plt.plot(range(1, epochs+1), results_bn[best_lr]['train_accs'], 'o-', label='With BN')\n    plt.xlabel('Epoch')\n    plt.ylabel('Training Accuracy')\n    plt.title('Training Accuracy Comparison')\n    plt.grid(True)\n    plt.legend()\n    \n    # Validation Loss comparison\n    plt.subplot(2, 2, 3)\n    plt.plot(range(1, epochs+1), results_non_bn[best_lr]['val_losses'], 'o-', label='Without BN')\n    plt.plot(range(1, epochs+1), results_bn[best_lr]['val_losses'], 'o-', label='With BN')\n    plt.xlabel('Epoch')\n    plt.ylabel('Validation Loss')\n    plt.title('Validation Loss Comparison')\n    plt.grid(True)\n    plt.legend()\n    \n    # Validation Accuracy comparison\n    plt.subplot(2, 2, 4)\n    plt.plot(range(1, epochs+1), results_non_bn[best_lr]['val_accs'], 'o-', label='Without BN')\n    plt.plot(range(1, epochs+1), results_bn[best_lr]['val_accs'], 'o-', label='With BN')\n    plt.xlabel('Epoch')\n    plt.ylabel('Validation Accuracy')\n    plt.title('Validation Accuracy Comparison')\n    plt.grid(True)\n    plt.legend()\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.savefig(os.path.join(figures_path, 'bn_vs_non_bn_comparison.png'), dpi=300)\n    plt.close()\n\n# Create loss landscape visualization as per project requirements\ndef plot_loss_landscape(results_non_bn, results_bn, learning_rates):\n    # Prepare batch loss data for non-BN models\n    non_bn_losses = []\n    for lr in learning_rates:\n        non_bn_losses.append(results_non_bn[lr]['batch_losses'])\n    \n    # Prepare batch loss data for BN models\n    bn_losses = []\n    for lr in learning_rates:\n        bn_losses.append(results_bn[lr]['batch_losses'])\n    \n    # Find the minimum length among all runs\n    min_length_non_bn = min(len(losses) for losses in non_bn_losses)\n    min_length_bn = min(len(losses) for losses in bn_losses)\n    min_length = min(min_length_non_bn, min_length_bn)\n    \n    # Trim all loss lists to the same length\n    non_bn_losses = [losses[:min_length] for losses in non_bn_losses]\n    bn_losses = [losses[:min_length] for losses in bn_losses]\n    \n    # Create min and max curves for non-BN\n    min_curve_non_bn = []\n    max_curve_non_bn = []\n    for step in range(min_length):\n        step_losses = [losses[step] for losses in non_bn_losses]\n        min_curve_non_bn.append(min(step_losses))\n        max_curve_non_bn.append(max(step_losses))\n    \n    # Create min and max curves for BN\n    min_curve_bn = []\n    max_curve_bn = []\n    for step in range(min_length):\n        step_losses = [losses[step] for losses in bn_losses]\n        min_curve_bn.append(min(step_losses))\n        max_curve_bn.append(max(step_losses))\n    \n    # Plot loss landscape for non-BN models\n    plt.figure(figsize=(10, 6))\n    steps = range(min_length)\n    plt.fill_between(steps, min_curve_non_bn, max_curve_non_bn, color='red', alpha=0.3)\n    plt.plot(steps, min_curve_non_bn, 'r-', linewidth=1, label='Min Loss')\n    plt.plot(steps, max_curve_non_bn, 'r--', linewidth=1, label='Max Loss')\n    plt.xlabel('Training Steps')\n    plt.ylabel('Batch Loss')\n    plt.title('Loss Landscape Without BatchNorm')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(figures_path, 'non_bn_loss_landscape.png'), dpi=300)\n    plt.close()\n    \n    # Plot loss landscape for BN models\n    plt.figure(figsize=(10, 6))\n    steps = range(min_length)\n    plt.fill_between(steps, min_curve_bn, max_curve_bn, color='blue', alpha=0.3)\n    plt.plot(steps, min_curve_bn, 'b-', linewidth=1, label='Min Loss')\n    plt.plot(steps, max_curve_bn, 'b--', linewidth=1, label='Max Loss')\n    plt.xlabel('Training Steps')\n    plt.ylabel('Batch Loss')\n    plt.title('Loss Landscape With BatchNorm')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(figures_path, 'bn_loss_landscape.png'), dpi=300)\n    plt.close()\n    \n    # Create combined loss landscape comparison\n    plt.figure(figsize=(12, 8))\n    steps = range(min_length)\n    \n    # Plot non-BN landscape\n    plt.fill_between(steps, min_curve_non_bn, max_curve_non_bn, color='red', alpha=0.3, label='Without BN')\n    plt.plot(steps, min_curve_non_bn, 'r-', linewidth=1, alpha=0.7)\n    plt.plot(steps, max_curve_non_bn, 'r--', linewidth=1, alpha=0.7)\n    \n    # Plot BN landscape\n    plt.fill_between(steps, min_curve_bn, max_curve_bn, color='blue', alpha=0.3, label='With BN')\n    plt.plot(steps, min_curve_bn, 'b-', linewidth=1, alpha=0.7)\n    plt.plot(steps, max_curve_bn, 'b--', linewidth=1, alpha=0.7)\n    \n    plt.xlabel('Training Steps')\n    plt.ylabel('Batch Loss')\n    plt.title('Batch Normalization Effect on Loss Landscape')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.legend()\n    \n    # Add annotations\n    plt.annotate('BN stabilizes loss landscape\\nreducing variance across learning rates', \n                xy=(min_length*0.6, (max_curve_non_bn[min_length//2] + min_curve_non_bn[min_length//2])/2),\n                xytext=(min_length*0.4, max_curve_non_bn[0]*0.8),\n                arrowprops=dict(facecolor='black', shrink=0.05),\n                fontsize=12)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(figures_path, 'bn_loss_landscape_comparison.png'), dpi=300)\n    plt.close()\n\n# Run the analysis\nif __name__ == '__main__':\n    analyze_bn_effect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T14:35:55.968980Z","iopub.execute_input":"2025-06-07T14:35:55.969356Z","iopub.status.idle":"2025-06-07T15:42:34.709853Z","shell.execute_reply.started":"2025-06-07T14:35:55.969331Z","shell.execute_reply":"2025-06-07T15:42:34.709022Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU count: 2\n\n==================================================\nTraining non-BatchNorm models\n==================================================\n\n==================== Training without BN (LR: 1e-03) ====================\n[Without BN] Epoch 1/20 (LR: 1e-03) | Train Loss: 1.7739, Train Acc: 0.3063 | Val Loss: 1.4425, Val Acc: 0.4477 | Time: 21.7s\n[Without BN] Epoch 2/20 (LR: 1e-03) | Train Loss: 1.2262, Train Acc: 0.5512 | Val Loss: 1.0553, Val Acc: 0.6202 | Time: 20.2s\n[Without BN] Epoch 3/20 (LR: 1e-03) | Train Loss: 0.9344, Train Acc: 0.6686 | Val Loss: 0.9420, Val Acc: 0.6737 | Time: 20.3s\n[Without BN] Epoch 4/20 (LR: 1e-03) | Train Loss: 0.7700, Train Acc: 0.7320 | Val Loss: 0.8052, Val Acc: 0.7195 | Time: 19.8s\n[Without BN] Epoch 5/20 (LR: 1e-03) | Train Loss: 0.6459, Train Acc: 0.7749 | Val Loss: 0.8381, Val Acc: 0.7257 | Time: 20.1s\n[Without BN] Epoch 6/20 (LR: 1e-03) | Train Loss: 0.5408, Train Acc: 0.8118 | Val Loss: 0.8160, Val Acc: 0.7435 | Time: 20.3s\n[Without BN] Epoch 7/20 (LR: 1e-03) | Train Loss: 0.4560, Train Acc: 0.8434 | Val Loss: 0.8076, Val Acc: 0.7421 | Time: 20.1s\n[Without BN] Epoch 8/20 (LR: 1e-03) | Train Loss: 0.3787, Train Acc: 0.8697 | Val Loss: 0.8219, Val Acc: 0.7548 | Time: 20.2s\n[Without BN] Epoch 9/20 (LR: 1e-03) | Train Loss: 0.3102, Train Acc: 0.8930 | Val Loss: 0.8846, Val Acc: 0.7581 | Time: 20.1s\n[Without BN] Epoch 10/20 (LR: 1e-03) | Train Loss: 0.2530, Train Acc: 0.9129 | Val Loss: 0.9185, Val Acc: 0.7521 | Time: 20.5s\n[Without BN] Epoch 11/20 (LR: 1e-03) | Train Loss: 0.2219, Train Acc: 0.9240 | Val Loss: 0.9445, Val Acc: 0.7539 | Time: 20.3s\n[Without BN] Epoch 12/20 (LR: 1e-03) | Train Loss: 0.1953, Train Acc: 0.9339 | Val Loss: 1.0274, Val Acc: 0.7558 | Time: 20.1s\n[Without BN] Epoch 13/20 (LR: 1e-03) | Train Loss: 0.1641, Train Acc: 0.9449 | Val Loss: 0.9792, Val Acc: 0.7621 | Time: 20.5s\n[Without BN] Epoch 14/20 (LR: 1e-03) | Train Loss: 0.1451, Train Acc: 0.9520 | Val Loss: 1.0555, Val Acc: 0.7715 | Time: 20.8s\n[Without BN] Epoch 15/20 (LR: 1e-03) | Train Loss: 0.1296, Train Acc: 0.9568 | Val Loss: 1.1080, Val Acc: 0.7689 | Time: 20.2s\n[Without BN] Epoch 16/20 (LR: 1e-03) | Train Loss: 0.1110, Train Acc: 0.9639 | Val Loss: 1.2044, Val Acc: 0.7608 | Time: 20.5s\n[Without BN] Epoch 17/20 (LR: 1e-03) | Train Loss: 0.1083, Train Acc: 0.9652 | Val Loss: 1.1613, Val Acc: 0.7613 | Time: 20.2s\n[Without BN] Epoch 18/20 (LR: 1e-03) | Train Loss: 0.0969, Train Acc: 0.9683 | Val Loss: 1.3266, Val Acc: 0.7532 | Time: 20.5s\n[Without BN] Epoch 19/20 (LR: 1e-03) | Train Loss: 0.0979, Train Acc: 0.9695 | Val Loss: 1.2226, Val Acc: 0.7603 | Time: 20.2s\n[Without BN] Epoch 20/20 (LR: 1e-03) | Train Loss: 0.0870, Train Acc: 0.9721 | Val Loss: 1.3038, Val Acc: 0.7676 | Time: 20.1s\nSaved best model to /kaggle/working/models/non_bn_model_lr_0.001.pth with val acc 0.7715\n\n==================== Training without BN (LR: 2e-03) ====================\n[Without BN] Epoch 1/20 (LR: 2e-03) | Train Loss: 2.3106, Train Acc: 0.0977 | Val Loss: 2.3027, Val Acc: 0.1000 | Time: 20.7s\n[Without BN] Epoch 2/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0973 | Val Loss: 2.3027, Val Acc: 0.1000 | Time: 20.8s\n[Without BN] Epoch 3/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0991 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.2s\n[Without BN] Epoch 4/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0992 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.6s\n[Without BN] Epoch 5/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0990 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.7s\n[Without BN] Epoch 6/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0961 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.4s\n[Without BN] Epoch 7/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.1005 | Val Loss: 2.3027, Val Acc: 0.1000 | Time: 20.5s\n[Without BN] Epoch 8/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0972 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.3s\n[Without BN] Epoch 9/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0976 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.4s\n[Without BN] Epoch 10/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0977 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.5s\n[Without BN] Epoch 11/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0974 | Val Loss: 2.3027, Val Acc: 0.1000 | Time: 20.3s\n[Without BN] Epoch 12/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0978 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.6s\n[Without BN] Epoch 13/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0980 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 21.0s\n[Without BN] Epoch 14/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0965 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.5s\n[Without BN] Epoch 15/20 (LR: 2e-03) | Train Loss: 2.3027, Train Acc: 0.0969 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.8s\n[Without BN] Epoch 16/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0984 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 21.2s\n[Without BN] Epoch 17/20 (LR: 2e-03) | Train Loss: 2.3027, Train Acc: 0.0972 | Val Loss: 2.3027, Val Acc: 0.1000 | Time: 20.6s\n[Without BN] Epoch 18/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0991 | Val Loss: 2.3027, Val Acc: 0.1000 | Time: 20.8s\n[Without BN] Epoch 19/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0981 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.8s\n[Without BN] Epoch 20/20 (LR: 2e-03) | Train Loss: 2.3028, Train Acc: 0.0983 | Val Loss: 2.3026, Val Acc: 0.1000 | Time: 20.7s\nSaved best model to /kaggle/working/models/non_bn_model_lr_0.002.pth with val acc 0.1000\n\n==================== Training without BN (LR: 1e-04) ====================\n[Without BN] Epoch 1/20 (LR: 1e-04) | Train Loss: 1.7280, Train Acc: 0.3556 | Val Loss: 1.4988, Val Acc: 0.4421 | Time: 21.0s\n[Without BN] Epoch 2/20 (LR: 1e-04) | Train Loss: 1.3263, Train Acc: 0.5159 | Val Loss: 1.2381, Val Acc: 0.5431 | Time: 20.9s\n[Without BN] Epoch 3/20 (LR: 1e-04) | Train Loss: 1.1280, Train Acc: 0.5897 | Val Loss: 1.0425, Val Acc: 0.6214 | Time: 21.5s\n[Without BN] Epoch 4/20 (LR: 1e-04) | Train Loss: 0.9805, Train Acc: 0.6464 | Val Loss: 0.9621, Val Acc: 0.6556 | Time: 22.1s\n[Without BN] Epoch 5/20 (LR: 1e-04) | Train Loss: 0.8427, Train Acc: 0.6991 | Val Loss: 0.8752, Val Acc: 0.6947 | Time: 22.3s\n[Without BN] Epoch 6/20 (LR: 1e-04) | Train Loss: 0.7202, Train Acc: 0.7449 | Val Loss: 0.7905, Val Acc: 0.7243 | Time: 22.2s\n[Without BN] Epoch 7/20 (LR: 1e-04) | Train Loss: 0.6078, Train Acc: 0.7840 | Val Loss: 0.7463, Val Acc: 0.7404 | Time: 22.5s\n[Without BN] Epoch 8/20 (LR: 1e-04) | Train Loss: 0.5077, Train Acc: 0.8203 | Val Loss: 0.7975, Val Acc: 0.7305 | Time: 22.8s\n[Without BN] Epoch 9/20 (LR: 1e-04) | Train Loss: 0.4017, Train Acc: 0.8588 | Val Loss: 0.7855, Val Acc: 0.7462 | Time: 22.4s\n[Without BN] Epoch 10/20 (LR: 1e-04) | Train Loss: 0.3051, Train Acc: 0.8942 | Val Loss: 0.8092, Val Acc: 0.7541 | Time: 22.6s\n[Without BN] Epoch 11/20 (LR: 1e-04) | Train Loss: 0.2198, Train Acc: 0.9241 | Val Loss: 0.8786, Val Acc: 0.7569 | Time: 22.7s\n[Without BN] Epoch 12/20 (LR: 1e-04) | Train Loss: 0.1577, Train Acc: 0.9454 | Val Loss: 0.8872, Val Acc: 0.7584 | Time: 22.1s\n[Without BN] Epoch 13/20 (LR: 1e-04) | Train Loss: 0.1138, Train Acc: 0.9610 | Val Loss: 1.0033, Val Acc: 0.7531 | Time: 22.3s\n[Without BN] Epoch 14/20 (LR: 1e-04) | Train Loss: 0.0838, Train Acc: 0.9715 | Val Loss: 1.0133, Val Acc: 0.7625 | Time: 22.6s\n[Without BN] Epoch 15/20 (LR: 1e-04) | Train Loss: 0.0675, Train Acc: 0.9770 | Val Loss: 1.1309, Val Acc: 0.7350 | Time: 22.2s\n[Without BN] Epoch 16/20 (LR: 1e-04) | Train Loss: 0.0697, Train Acc: 0.9758 | Val Loss: 1.1285, Val Acc: 0.7682 | Time: 22.5s\n[Without BN] Epoch 17/20 (LR: 1e-04) | Train Loss: 0.0572, Train Acc: 0.9803 | Val Loss: 1.0939, Val Acc: 0.7652 | Time: 22.4s\n[Without BN] Epoch 18/20 (LR: 1e-04) | Train Loss: 0.0507, Train Acc: 0.9821 | Val Loss: 1.1858, Val Acc: 0.7707 | Time: 22.2s\n[Without BN] Epoch 19/20 (LR: 1e-04) | Train Loss: 0.0470, Train Acc: 0.9837 | Val Loss: 1.3101, Val Acc: 0.7568 | Time: 22.3s\n[Without BN] Epoch 20/20 (LR: 1e-04) | Train Loss: 0.0436, Train Acc: 0.9846 | Val Loss: 1.2692, Val Acc: 0.7560 | Time: 22.6s\nSaved best model to /kaggle/working/models/non_bn_model_lr_0.0001.pth with val acc 0.7707\n\n==================== Training without BN (LR: 5e-04) ====================\n[Without BN] Epoch 1/20 (LR: 5e-04) | Train Loss: 1.7015, Train Acc: 0.3463 | Val Loss: 1.3768, Val Acc: 0.4585 | Time: 22.9s\n[Without BN] Epoch 2/20 (LR: 5e-04) | Train Loss: 1.1125, Train Acc: 0.5939 | Val Loss: 0.9523, Val Acc: 0.6591 | Time: 22.4s\n[Without BN] Epoch 3/20 (LR: 5e-04) | Train Loss: 0.8242, Train Acc: 0.7105 | Val Loss: 0.8388, Val Acc: 0.7129 | Time: 22.7s\n[Without BN] Epoch 4/20 (LR: 5e-04) | Train Loss: 0.6486, Train Acc: 0.7747 | Val Loss: 0.7918, Val Acc: 0.7335 | Time: 23.0s\n[Without BN] Epoch 5/20 (LR: 5e-04) | Train Loss: 0.5035, Train Acc: 0.8270 | Val Loss: 0.7439, Val Acc: 0.7597 | Time: 22.8s\n[Without BN] Epoch 6/20 (LR: 5e-04) | Train Loss: 0.3754, Train Acc: 0.8717 | Val Loss: 0.7218, Val Acc: 0.7765 | Time: 23.1s\n[Without BN] Epoch 7/20 (LR: 5e-04) | Train Loss: 0.2765, Train Acc: 0.9038 | Val Loss: 0.7765, Val Acc: 0.7744 | Time: 23.0s\n[Without BN] Epoch 8/20 (LR: 5e-04) | Train Loss: 0.2034, Train Acc: 0.9297 | Val Loss: 0.8671, Val Acc: 0.7633 | Time: 23.1s\n[Without BN] Epoch 9/20 (LR: 5e-04) | Train Loss: 0.1542, Train Acc: 0.9492 | Val Loss: 0.9064, Val Acc: 0.7787 | Time: 23.1s\n[Without BN] Epoch 10/20 (LR: 5e-04) | Train Loss: 0.1185, Train Acc: 0.9602 | Val Loss: 1.0907, Val Acc: 0.7779 | Time: 23.1s\n[Without BN] Epoch 11/20 (LR: 5e-04) | Train Loss: 0.0920, Train Acc: 0.9696 | Val Loss: 1.0434, Val Acc: 0.7779 | Time: 23.4s\n[Without BN] Epoch 12/20 (LR: 5e-04) | Train Loss: 0.0909, Train Acc: 0.9697 | Val Loss: 0.9818, Val Acc: 0.7798 | Time: 22.9s\n[Without BN] Epoch 13/20 (LR: 5e-04) | Train Loss: 0.0726, Train Acc: 0.9759 | Val Loss: 1.1739, Val Acc: 0.7836 | Time: 23.2s\n[Without BN] Epoch 14/20 (LR: 5e-04) | Train Loss: 0.0654, Train Acc: 0.9795 | Val Loss: 1.1847, Val Acc: 0.7776 | Time: 23.4s\n[Without BN] Epoch 15/20 (LR: 5e-04) | Train Loss: 0.0668, Train Acc: 0.9781 | Val Loss: 1.1817, Val Acc: 0.7825 | Time: 23.6s\n[Without BN] Epoch 16/20 (LR: 5e-04) | Train Loss: 0.0568, Train Acc: 0.9815 | Val Loss: 1.1321, Val Acc: 0.7770 | Time: 23.1s\n[Without BN] Epoch 17/20 (LR: 5e-04) | Train Loss: 0.0539, Train Acc: 0.9828 | Val Loss: 1.1152, Val Acc: 0.7858 | Time: 23.3s\n[Without BN] Epoch 18/20 (LR: 5e-04) | Train Loss: 0.0511, Train Acc: 0.9831 | Val Loss: 1.1304, Val Acc: 0.7680 | Time: 23.5s\n[Without BN] Epoch 19/20 (LR: 5e-04) | Train Loss: 0.0553, Train Acc: 0.9828 | Val Loss: 1.1821, Val Acc: 0.7870 | Time: 23.2s\n[Without BN] Epoch 20/20 (LR: 5e-04) | Train Loss: 0.0464, Train Acc: 0.9851 | Val Loss: 1.2162, Val Acc: 0.7776 | Time: 23.3s\nSaved best model to /kaggle/working/models/non_bn_model_lr_0.0005.pth with val acc 0.7870\n\n==================================================\nTraining BatchNorm models\n==================================================\n\n==================== Training with BN (LR: 1e-03) ====================\n[With BN] Epoch 1/20 (LR: 1e-03) | Train Loss: 1.2832, Train Acc: 0.5306 | Val Loss: 1.1879, Val Acc: 0.5870 | Time: 27.3s\n[With BN] Epoch 2/20 (LR: 1e-03) | Train Loss: 0.8295, Train Acc: 0.7078 | Val Loss: 0.8347, Val Acc: 0.7143 | Time: 27.3s\n[With BN] Epoch 3/20 (LR: 1e-03) | Train Loss: 0.6453, Train Acc: 0.7767 | Val Loss: 0.8768, Val Acc: 0.7122 | Time: 28.0s\n[With BN] Epoch 4/20 (LR: 1e-03) | Train Loss: 0.5198, Train Acc: 0.8189 | Val Loss: 0.7171, Val Acc: 0.7596 | Time: 27.7s\n[With BN] Epoch 5/20 (LR: 1e-03) | Train Loss: 0.4287, Train Acc: 0.8526 | Val Loss: 0.5498, Val Acc: 0.8114 | Time: 27.5s\n[With BN] Epoch 6/20 (LR: 1e-03) | Train Loss: 0.3406, Train Acc: 0.8835 | Val Loss: 0.6220, Val Acc: 0.7927 | Time: 27.5s\n[With BN] Epoch 7/20 (LR: 1e-03) | Train Loss: 0.2663, Train Acc: 0.9086 | Val Loss: 0.7057, Val Acc: 0.7849 | Time: 27.7s\n[With BN] Epoch 8/20 (LR: 1e-03) | Train Loss: 0.2061, Train Acc: 0.9288 | Val Loss: 0.5985, Val Acc: 0.8211 | Time: 27.5s\n[With BN] Epoch 9/20 (LR: 1e-03) | Train Loss: 0.1633, Train Acc: 0.9433 | Val Loss: 0.6917, Val Acc: 0.8109 | Time: 27.6s\n[With BN] Epoch 10/20 (LR: 1e-03) | Train Loss: 0.1349, Train Acc: 0.9524 | Val Loss: 0.7252, Val Acc: 0.8086 | Time: 27.8s\n[With BN] Epoch 11/20 (LR: 1e-03) | Train Loss: 0.1044, Train Acc: 0.9634 | Val Loss: 0.7105, Val Acc: 0.8150 | Time: 27.8s\n[With BN] Epoch 12/20 (LR: 1e-03) | Train Loss: 0.0938, Train Acc: 0.9675 | Val Loss: 0.7936, Val Acc: 0.7998 | Time: 27.7s\n[With BN] Epoch 13/20 (LR: 1e-03) | Train Loss: 0.0823, Train Acc: 0.9712 | Val Loss: 0.7354, Val Acc: 0.8229 | Time: 27.5s\n[With BN] Epoch 14/20 (LR: 1e-03) | Train Loss: 0.0686, Train Acc: 0.9763 | Val Loss: 0.7607, Val Acc: 0.8213 | Time: 27.8s\n[With BN] Epoch 15/20 (LR: 1e-03) | Train Loss: 0.0639, Train Acc: 0.9775 | Val Loss: 0.8532, Val Acc: 0.8190 | Time: 27.9s\n[With BN] Epoch 16/20 (LR: 1e-03) | Train Loss: 0.0666, Train Acc: 0.9769 | Val Loss: 0.7816, Val Acc: 0.8199 | Time: 27.5s\n[With BN] Epoch 17/20 (LR: 1e-03) | Train Loss: 0.0513, Train Acc: 0.9820 | Val Loss: 0.7906, Val Acc: 0.8302 | Time: 27.7s\n[With BN] Epoch 18/20 (LR: 1e-03) | Train Loss: 0.0491, Train Acc: 0.9833 | Val Loss: 0.8720, Val Acc: 0.8111 | Time: 27.6s\n[With BN] Epoch 19/20 (LR: 1e-03) | Train Loss: 0.0480, Train Acc: 0.9832 | Val Loss: 0.8431, Val Acc: 0.8145 | Time: 28.0s\n[With BN] Epoch 20/20 (LR: 1e-03) | Train Loss: 0.0434, Train Acc: 0.9849 | Val Loss: 0.9252, Val Acc: 0.8085 | Time: 27.7s\nSaved best model to /kaggle/working/models/bn_model_lr_0.001.pth with val acc 0.8302\n\n==================== Training with BN (LR: 2e-03) ====================\n[With BN] Epoch 1/20 (LR: 2e-03) | Train Loss: 1.4015, Train Acc: 0.4806 | Val Loss: 1.2115, Val Acc: 0.5753 | Time: 27.8s\n[With BN] Epoch 2/20 (LR: 2e-03) | Train Loss: 0.9356, Train Acc: 0.6669 | Val Loss: 0.8567, Val Acc: 0.7002 | Time: 27.7s\n[With BN] Epoch 3/20 (LR: 2e-03) | Train Loss: 0.7379, Train Acc: 0.7424 | Val Loss: 0.8961, Val Acc: 0.6890 | Time: 28.0s\n[With BN] Epoch 4/20 (LR: 2e-03) | Train Loss: 0.6069, Train Acc: 0.7895 | Val Loss: 0.8513, Val Acc: 0.7253 | Time: 28.0s\n[With BN] Epoch 5/20 (LR: 2e-03) | Train Loss: 0.4993, Train Acc: 0.8280 | Val Loss: 0.6681, Val Acc: 0.7804 | Time: 28.0s\n[With BN] Epoch 6/20 (LR: 2e-03) | Train Loss: 0.3952, Train Acc: 0.8637 | Val Loss: 0.6777, Val Acc: 0.7859 | Time: 27.8s\n[With BN] Epoch 7/20 (LR: 2e-03) | Train Loss: 0.3208, Train Acc: 0.8886 | Val Loss: 0.7451, Val Acc: 0.7709 | Time: 28.0s\n[With BN] Epoch 8/20 (LR: 2e-03) | Train Loss: 0.2475, Train Acc: 0.9148 | Val Loss: 0.7808, Val Acc: 0.7720 | Time: 27.9s\n[With BN] Epoch 9/20 (LR: 2e-03) | Train Loss: 0.1935, Train Acc: 0.9349 | Val Loss: 0.6175, Val Acc: 0.8199 | Time: 28.1s\n[With BN] Epoch 10/20 (LR: 2e-03) | Train Loss: 0.1558, Train Acc: 0.9457 | Val Loss: 0.6514, Val Acc: 0.8150 | Time: 28.1s\n[With BN] Epoch 11/20 (LR: 2e-03) | Train Loss: 0.1250, Train Acc: 0.9573 | Val Loss: 0.7293, Val Acc: 0.8146 | Time: 28.3s\n[With BN] Epoch 12/20 (LR: 2e-03) | Train Loss: 0.1007, Train Acc: 0.9667 | Val Loss: 0.7040, Val Acc: 0.8126 | Time: 28.2s\n[With BN] Epoch 13/20 (LR: 2e-03) | Train Loss: 0.0897, Train Acc: 0.9693 | Val Loss: 0.7884, Val Acc: 0.8172 | Time: 28.2s\n[With BN] Epoch 14/20 (LR: 2e-03) | Train Loss: 0.0812, Train Acc: 0.9725 | Val Loss: 0.7949, Val Acc: 0.8049 | Time: 28.1s\n[With BN] Epoch 15/20 (LR: 2e-03) | Train Loss: 0.0667, Train Acc: 0.9775 | Val Loss: 0.8290, Val Acc: 0.8106 | Time: 28.6s\n[With BN] Epoch 16/20 (LR: 2e-03) | Train Loss: 0.0668, Train Acc: 0.9775 | Val Loss: 0.8545, Val Acc: 0.8146 | Time: 28.2s\n[With BN] Epoch 17/20 (LR: 2e-03) | Train Loss: 0.0597, Train Acc: 0.9799 | Val Loss: 0.8159, Val Acc: 0.8138 | Time: 28.2s\n[With BN] Epoch 18/20 (LR: 2e-03) | Train Loss: 0.0541, Train Acc: 0.9818 | Val Loss: 0.8511, Val Acc: 0.8168 | Time: 28.0s\n[With BN] Epoch 19/20 (LR: 2e-03) | Train Loss: 0.0485, Train Acc: 0.9833 | Val Loss: 0.8381, Val Acc: 0.8220 | Time: 28.1s\n[With BN] Epoch 20/20 (LR: 2e-03) | Train Loss: 0.0523, Train Acc: 0.9826 | Val Loss: 0.9571, Val Acc: 0.8062 | Time: 28.4s\nSaved best model to /kaggle/working/models/bn_model_lr_0.002.pth with val acc 0.8220\n\n==================== Training with BN (LR: 1e-04) ====================\n[With BN] Epoch 1/20 (LR: 1e-04) | Train Loss: 1.3286, Train Acc: 0.5246 | Val Loss: 1.1250, Val Acc: 0.6039 | Time: 28.3s\n[With BN] Epoch 2/20 (LR: 1e-04) | Train Loss: 0.8914, Train Acc: 0.6863 | Val Loss: 1.0126, Val Acc: 0.6503 | Time: 28.6s\n[With BN] Epoch 3/20 (LR: 1e-04) | Train Loss: 0.6642, Train Acc: 0.7687 | Val Loss: 0.8710, Val Acc: 0.6962 | Time: 28.3s\n[With BN] Epoch 4/20 (LR: 1e-04) | Train Loss: 0.4866, Train Acc: 0.8310 | Val Loss: 0.9877, Val Acc: 0.6786 | Time: 28.3s\n[With BN] Epoch 5/20 (LR: 1e-04) | Train Loss: 0.3333, Train Acc: 0.8855 | Val Loss: 0.9559, Val Acc: 0.7037 | Time: 28.5s\n[With BN] Epoch 6/20 (LR: 1e-04) | Train Loss: 0.2236, Train Acc: 0.9246 | Val Loss: 0.9288, Val Acc: 0.7253 | Time: 28.4s\n[With BN] Epoch 7/20 (LR: 1e-04) | Train Loss: 0.1595, Train Acc: 0.9473 | Val Loss: 1.1567, Val Acc: 0.6923 | Time: 28.6s\n[With BN] Epoch 8/20 (LR: 1e-04) | Train Loss: 0.1263, Train Acc: 0.9568 | Val Loss: 1.0322, Val Acc: 0.7150 | Time: 28.4s\n[With BN] Epoch 9/20 (LR: 1e-04) | Train Loss: 0.1082, Train Acc: 0.9635 | Val Loss: 1.2362, Val Acc: 0.7077 | Time: 28.6s\n[With BN] Epoch 10/20 (LR: 1e-04) | Train Loss: 0.1023, Train Acc: 0.9657 | Val Loss: 1.2397, Val Acc: 0.6954 | Time: 28.6s\n[With BN] Epoch 11/20 (LR: 1e-04) | Train Loss: 0.0878, Train Acc: 0.9700 | Val Loss: 1.0989, Val Acc: 0.7268 | Time: 28.9s\n[With BN] Epoch 12/20 (LR: 1e-04) | Train Loss: 0.0729, Train Acc: 0.9753 | Val Loss: 1.2017, Val Acc: 0.7185 | Time: 28.9s\n[With BN] Epoch 13/20 (LR: 1e-04) | Train Loss: 0.0769, Train Acc: 0.9728 | Val Loss: 1.3096, Val Acc: 0.7129 | Time: 28.9s\n[With BN] Epoch 14/20 (LR: 1e-04) | Train Loss: 0.0762, Train Acc: 0.9738 | Val Loss: 1.2206, Val Acc: 0.7208 | Time: 28.7s\n[With BN] Epoch 15/20 (LR: 1e-04) | Train Loss: 0.0617, Train Acc: 0.9785 | Val Loss: 1.2720, Val Acc: 0.7252 | Time: 28.8s\n[With BN] Epoch 16/20 (LR: 1e-04) | Train Loss: 0.0642, Train Acc: 0.9779 | Val Loss: 1.2849, Val Acc: 0.7170 | Time: 28.8s\n[With BN] Epoch 17/20 (LR: 1e-04) | Train Loss: 0.0474, Train Acc: 0.9836 | Val Loss: 1.2540, Val Acc: 0.7265 | Time: 28.8s\n[With BN] Epoch 18/20 (LR: 1e-04) | Train Loss: 0.0649, Train Acc: 0.9779 | Val Loss: 1.3208, Val Acc: 0.7208 | Time: 28.7s\n[With BN] Epoch 19/20 (LR: 1e-04) | Train Loss: 0.0505, Train Acc: 0.9827 | Val Loss: 1.3009, Val Acc: 0.7267 | Time: 29.2s\n[With BN] Epoch 20/20 (LR: 1e-04) | Train Loss: 0.0507, Train Acc: 0.9822 | Val Loss: 1.2715, Val Acc: 0.7317 | Time: 28.4s\nSaved best model to /kaggle/working/models/bn_model_lr_0.0001.pth with val acc 0.7317\n\n==================== Training with BN (LR: 5e-04) ====================\n[With BN] Epoch 1/20 (LR: 5e-04) | Train Loss: 1.2175, Train Acc: 0.5606 | Val Loss: 1.1497, Val Acc: 0.6032 | Time: 28.8s\n[With BN] Epoch 2/20 (LR: 5e-04) | Train Loss: 0.7783, Train Acc: 0.7274 | Val Loss: 0.7646, Val Acc: 0.7296 | Time: 28.9s\n[With BN] Epoch 3/20 (LR: 5e-04) | Train Loss: 0.6028, Train Acc: 0.7931 | Val Loss: 0.9457, Val Acc: 0.6928 | Time: 28.6s\n[With BN] Epoch 4/20 (LR: 5e-04) | Train Loss: 0.4798, Train Acc: 0.8353 | Val Loss: 0.7131, Val Acc: 0.7650 | Time: 28.6s\n[With BN] Epoch 5/20 (LR: 5e-04) | Train Loss: 0.3834, Train Acc: 0.8666 | Val Loss: 0.7340, Val Acc: 0.7614 | Time: 28.6s\n[With BN] Epoch 6/20 (LR: 5e-04) | Train Loss: 0.3005, Train Acc: 0.8945 | Val Loss: 0.6360, Val Acc: 0.7906 | Time: 28.7s\n[With BN] Epoch 7/20 (LR: 5e-04) | Train Loss: 0.2268, Train Acc: 0.9206 | Val Loss: 0.7098, Val Acc: 0.7880 | Time: 28.6s\n[With BN] Epoch 8/20 (LR: 5e-04) | Train Loss: 0.1741, Train Acc: 0.9397 | Val Loss: 0.7859, Val Acc: 0.7766 | Time: 28.1s\n[With BN] Epoch 9/20 (LR: 5e-04) | Train Loss: 0.1381, Train Acc: 0.9522 | Val Loss: 0.7858, Val Acc: 0.7943 | Time: 28.4s\n[With BN] Epoch 10/20 (LR: 5e-04) | Train Loss: 0.1203, Train Acc: 0.9576 | Val Loss: 0.8527, Val Acc: 0.7863 | Time: 28.6s\n[With BN] Epoch 11/20 (LR: 5e-04) | Train Loss: 0.0898, Train Acc: 0.9691 | Val Loss: 0.8900, Val Acc: 0.7760 | Time: 28.5s\n[With BN] Epoch 12/20 (LR: 5e-04) | Train Loss: 0.0881, Train Acc: 0.9693 | Val Loss: 0.9523, Val Acc: 0.7846 | Time: 28.4s\n[With BN] Epoch 13/20 (LR: 5e-04) | Train Loss: 0.0757, Train Acc: 0.9735 | Val Loss: 0.8127, Val Acc: 0.8098 | Time: 28.3s\n[With BN] Epoch 14/20 (LR: 5e-04) | Train Loss: 0.0695, Train Acc: 0.9756 | Val Loss: 0.9043, Val Acc: 0.7961 | Time: 29.0s\n[With BN] Epoch 15/20 (LR: 5e-04) | Train Loss: 0.0677, Train Acc: 0.9761 | Val Loss: 0.8133, Val Acc: 0.8135 | Time: 29.1s\n[With BN] Epoch 16/20 (LR: 5e-04) | Train Loss: 0.0525, Train Acc: 0.9822 | Val Loss: 0.9148, Val Acc: 0.8010 | Time: 28.7s\n[With BN] Epoch 17/20 (LR: 5e-04) | Train Loss: 0.0588, Train Acc: 0.9799 | Val Loss: 0.9112, Val Acc: 0.8037 | Time: 28.4s\n[With BN] Epoch 18/20 (LR: 5e-04) | Train Loss: 0.0509, Train Acc: 0.9819 | Val Loss: 0.8883, Val Acc: 0.8136 | Time: 29.1s\n[With BN] Epoch 19/20 (LR: 5e-04) | Train Loss: 0.0526, Train Acc: 0.9815 | Val Loss: 0.8333, Val Acc: 0.8141 | Time: 28.9s\n[With BN] Epoch 20/20 (LR: 5e-04) | Train Loss: 0.0440, Train Acc: 0.9849 | Val Loss: 0.8737, Val Acc: 0.8176 | Time: 28.7s\nSaved best model to /kaggle/working/models/bn_model_lr_0.0005.pth with val acc 0.8176\n\nCreating performance comparison plots...\n\nCreating loss landscape plots...\n\nAnalysis complete! All outputs saved to /kaggle/working\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}